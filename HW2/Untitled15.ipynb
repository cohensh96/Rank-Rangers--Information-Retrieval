{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAuy5taOehnv",
        "outputId": "a0976692-8fb6-4853-80c2-022035b2411e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling: https://www.bundesgesundheitsministerium.de/en/ (ID: 1)\n",
            "Crawling: https://www.bundesgesundheitsministerium.de/en/international/european-health-policy.html (ID: 2)\n",
            "Crawling: https://www.bundesgesundheitsministerium.de/en/ministry/leadership/st-dr-tomas-steffen.html (ID: 3)\n",
            "Crawling: https://www.bundesgesundheitsministerium.de/en/topics.html (ID: 4)\n",
            "Crawling: https://www.bundesgesundheitsministerium.de/en/#c29679 (ID: 5)\n",
            "Crawling: https://www.bundesgesundheitsministerium.de/en/topics/digitalisation/digitalisation-in-healthcare.html (ID: 6)\n",
            "Crawling: https://www.bundesgesundheitsministerium.de/en/#c27721 (ID: 7)\n",
            "Crawling: https://www.bundesgesundheitsministerium.de/en/#c31215 (ID: 8)\n",
            "Crawling: https://www.bundesgesundheitsministerium.de/en/data-protection.html (ID: 9)\n",
            "Crawling: https://www.bundesgesundheitsministerium.de/en/#c29680 (ID: 10)\n",
            "Crawling: https://www.bundesgesundheitsministerium.de/en/topics/drinking-water.html (ID: 11)\n",
            "Crawling: https://www.bundesgesundheitsministerium.de/en/#c28661 (ID: 12)\n",
            "Crawling: https://www.bundesgesundheitsministerium.de/en/#c28712 (ID: 13)\n",
            "Crawling: https://www.bundesgesundheitsministerium.de/en/topics/digitalisation/digitalisation-strategy.html (ID: 14)\n",
            "Crawling: https://www.bundesgesundheitsministerium.de/en/user-information.html (ID: 15)\n",
            "Crawling: https://www.bundesgesundheitsministerium.de/en/#c27724 (ID: 16)\n",
            "Crawling: https://www.bundesgesundheitsministerium.de/en/#c28665 (ID: 17)\n",
            "Crawling: https://www.bundesgesundheitsministerium.de/en/ministry.html (ID: 18)\n",
            "Crawling: https://www.bundesgesundheitsministerium.de/en/ministry/leadership/stin-dr-antje-draheim.html (ID: 19)\n",
            "Crawling: https://www.bundesgesundheitsministerium.de/en/ministry/tasks-and-organisation/organisation-chart.html (ID: 20)\n",
            "Results saved to 'results.xlsx'.\n",
            "TF-IDF results for the query have been appended to the Excel file.\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Untitled10.ipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1TycP9vjnUW4e8C5vTZXCLxlLJhG2MxEe\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "from urllib.parse import urljoin\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import math  # For log in IDF\n",
        "import pandas as pd\n",
        "\n",
        "class SearchEngine:\n",
        "    def __init__(self):\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.stop_words = {\n",
        "            'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any',\n",
        "            'are', 'aren', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between',\n",
        "            'both', 'but', 'by', 'can', 'could', 'did', 'do', 'does', 'doing', 'don', 'down', 'during',\n",
        "            'each', 'few', 'for', 'from', 'further', 'had', 'has', 'have', 'having', 'he', 'her',\n",
        "            'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is',\n",
        "            'it', 'its', 'itself', 'just', 'me', 'more', 'most', 'my', 'myself', 'no', 'nor', 'not',\n",
        "            'now', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out',\n",
        "            'over', 'own', 's', 'same', 'she', 'should', 'so', 'some', 'such', 'than', 'that', 'the',\n",
        "            'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
        "            'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'we', 'were', 'what', 'when',\n",
        "            'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'you', 'your', 'yours',\n",
        "            'yourself', 'yourselves'\n",
        "        }\n",
        "        self.url_to_id = {}\n",
        "        self.id_to_url = {}\n",
        "        self.next_id = 1\n",
        "\n",
        "        # inverted_index[word][doc_id] = count of word in doc\n",
        "        self.inverted_index = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "        # global_index[word][url] = count of word in url\n",
        "        self.global_index = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    def get_url_id(self, url):\n",
        "        \"\"\"Map URLs to unique IDs\"\"\"\n",
        "        if url not in self.url_to_id:\n",
        "            self.url_to_id[url] = self.next_id\n",
        "            self.id_to_url[self.next_id] = url\n",
        "            self.next_id += 1\n",
        "        return self.url_to_id[url]\n",
        "\n",
        "    def fetch_page(self, url):\n",
        "        \"\"\"Fetch the HTML page content\"\"\"\n",
        "        try:\n",
        "            time.sleep(1)\n",
        "            response = requests.get(url, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                return BeautifulSoup(response.text, 'html.parser')\n",
        "            else:\n",
        "                print(f\"Failed to fetch {url}: Status code {response.status_code}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {url}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def get_links(self, soup, base_url):\n",
        "        \"\"\"Extract links from a page\"\"\"\n",
        "        links = set()\n",
        "        if soup:\n",
        "            for link in soup.find_all('a', href=True):\n",
        "                url = link['href']\n",
        "                absolute_url = urljoin(base_url, url)\n",
        "                if absolute_url.startswith(base_url):\n",
        "                    links.add(absolute_url)\n",
        "        return links\n",
        "\n",
        "    def index_words(self, soup, url):\n",
        "        \"\"\"\n",
        "        Index words from a page, storing the stemmed form.\n",
        "        Returns a dictionary of {word: count_in_this_document}.\n",
        "        \"\"\"\n",
        "        index = defaultdict(int)\n",
        "        url_id = self.get_url_id(url)\n",
        "\n",
        "        if soup:\n",
        "            words = re.findall(r'\\w+', soup.get_text())\n",
        "            for word in words:\n",
        "                word = word.lower()\n",
        "                if word not in self.stop_words:\n",
        "                    stemmed_word = self.stemmer.stem(word)\n",
        "\n",
        "                    # Increase counters in the local index (for this page)\n",
        "                    index[stemmed_word] += 1\n",
        "\n",
        "                    # Update the global/inverted index\n",
        "                    self.inverted_index[stemmed_word][url_id] += 1\n",
        "                    self.global_index[stemmed_word][url] += 1\n",
        "\n",
        "        return dict(index)\n",
        "\n",
        "    def crawl_and_index(self, start_url, max_pages=5):\n",
        "        \"\"\"Crawl and index pages. Returns a dict: {doc_id: {word: count}}\"\"\"\n",
        "        visited = set()\n",
        "        to_visit = {start_url}\n",
        "        page_indexes = {}\n",
        "\n",
        "        while to_visit and len(visited) < max_pages:\n",
        "            url = to_visit.pop()\n",
        "            if url in visited:\n",
        "                continue\n",
        "\n",
        "            print(f\"Crawling: {url} (ID: {self.get_url_id(url)})\")\n",
        "            soup = self.fetch_page(url)\n",
        "            if soup:\n",
        "                page_indexes[self.get_url_id(url)] = self.index_words(soup, url)\n",
        "                visited.add(url)\n",
        "                links = self.get_links(soup, start_url)\n",
        "                to_visit.update(links - visited)\n",
        "\n",
        "        return page_indexes\n",
        "\n",
        "    def save_to_excel(self, page_indexes, filename=\"results.xlsx\"):\n",
        "        \"\"\"\n",
        "        Save:\n",
        "         1) URL Mappings\n",
        "         2) Page Word Counts\n",
        "         3) Top 15 Inverted Index\n",
        "        \"\"\"\n",
        "        # 1) URL Mappings sheet\n",
        "        url_mappings = [{\"URL ID\": url_id, \"URL\": url} for url, url_id in self.url_to_id.items()]\n",
        "        url_df = pd.DataFrame(url_mappings)\n",
        "\n",
        "        # 2) Page Word Counts sheet\n",
        "        rows = []\n",
        "        for url_id, index in page_indexes.items():\n",
        "            for word, count in index.items():\n",
        "                rows.append({\"URL ID\": url_id, \"Word\": word, \"Count\": count})\n",
        "        counts_df = pd.DataFrame(rows)\n",
        "\n",
        "        # 3) Calculate global word frequencies to find top 15\n",
        "        word_frequencies = defaultdict(int)\n",
        "        for word, docs in self.inverted_index.items():\n",
        "            word_frequencies[word] = sum(docs.values())\n",
        "\n",
        "        top_words = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)[:15]\n",
        "        top_words_set = {word for word, _ in top_words}\n",
        "\n",
        "        # Filter the inverted index for the top 15 words\n",
        "        inverted_rows = []\n",
        "        for word, docs in self.inverted_index.items():\n",
        "            if word in top_words_set:\n",
        "                document_ids = \", \".join(map(str, docs.keys()))\n",
        "                inverted_rows.append({\"Word\": word, \"Document IDs\": document_ids})\n",
        "        inverted_df = pd.DataFrame(inverted_rows)\n",
        "\n",
        "        # Save to Excel (initial sheets)\n",
        "        with pd.ExcelWriter(filename, engine=\"openpyxl\") as writer:\n",
        "            url_df.to_excel(writer, index=False, sheet_name=\"URL Mappings\")\n",
        "            counts_df.to_excel(writer, index=False, sheet_name=\"Page Word Counts\")\n",
        "            inverted_df.to_excel(writer, index=False, sheet_name=\"Top 15 Inverted Index\")\n",
        "\n",
        "        print(f\"Results saved to '{filename}'.\")\n",
        "\n",
        "    def calculate_tf_idf_for_query(self, query, page_indexes, filename=\"results.xlsx\"):\n",
        "        \"\"\"\n",
        "        1) Preprocess query (tokenize, remove stopwords, stemming).\n",
        "        2) For each query word (concept), for each document:\n",
        "             - Compute TF\n",
        "             - Compute IDF\n",
        "             - Compute TF-IDF\n",
        "        3) Save detailed results (doc_id, url, concept, TF, IDF, TF-IDF) to a new sheet in the Excel file\n",
        "           with 10 decimal places.\n",
        "        \"\"\"\n",
        "        # 1) Preprocess the query\n",
        "        query_words = re.findall(r'\\w+', query.lower())\n",
        "        processed_query = []\n",
        "        for qw in query_words:\n",
        "            if qw not in self.stop_words:\n",
        "                stemmed_qw = self.stemmer.stem(qw)\n",
        "                processed_query.append(stemmed_qw)\n",
        "\n",
        "        # 2) Collect stats for TF-IDF\n",
        "        N = len(page_indexes)  # total number of documents\n",
        "        doc_total_words = {}\n",
        "        for doc_id, index_dict in page_indexes.items():\n",
        "            total_count = sum(index_dict.values())\n",
        "            doc_total_words[doc_id] = total_count\n",
        "\n",
        "        # 3) Compute TF, IDF, TF-IDF\n",
        "        tf_idf_rows = []\n",
        "        for doc_id, index_dict in page_indexes.items():\n",
        "            url = self.id_to_url[doc_id]\n",
        "            total_words_in_doc = doc_total_words[doc_id]\n",
        "\n",
        "            for qw in processed_query:\n",
        "                freq_in_doc = self.inverted_index[qw].get(doc_id, 0)\n",
        "                tf = freq_in_doc / total_words_in_doc if total_words_in_doc > 0 else 0.0\n",
        "\n",
        "\n",
        "                df = len(self.inverted_index[qw].keys())  # number of docs containing qw\n",
        "                idf = math.log(N / df, 10) if df > 0 else 0.0\n",
        "                tf_idf = tf * idf\n",
        "\n",
        "                tf_idf_rows.append({\n",
        "                    \"Document ID\": doc_id,\n",
        "                    \"URL\": url,\n",
        "                    \"Query Concept\": qw,\n",
        "                    \"TF\": tf,\n",
        "                    \"IDF\": idf,\n",
        "                    \"TF-IDF\": tf_idf\n",
        "                })\n",
        "\n",
        "        # 4) Create a DataFrame and append to the existing Excel file,\n",
        "        #    ensuring 10 decimal places in numeric columns.\n",
        "        tf_idf_df = pd.DataFrame(tf_idf_rows)\n",
        "\n",
        "        with pd.ExcelWriter(filename, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
        "            # The 'float_format' parameter ensures up to 10 digits after the decimal point\n",
        "            tf_idf_df.to_excel(\n",
        "                writer,\n",
        "                index=False,\n",
        "                sheet_name=\"Query TF-IDF\",\n",
        "                float_format=\"%.10f\"  # ensures 10 places\n",
        "            )\n",
        "\n",
        "        print(\"TF-IDF results for the query have been appended to the Excel file.\")\n",
        "\n",
        "def main():\n",
        "    search_engine = SearchEngine()\n",
        "\n",
        "    # 1) Crawl up to 20 pages\n",
        "    start_url = \"https://www.bundesgesundheitsministerium.de/en/\"\n",
        "    page_indexes = search_engine.crawl_and_index(start_url, max_pages=20)\n",
        "\n",
        "    # 2) Save the initial results\n",
        "    search_engine.save_to_excel(page_indexes, \"results.xlsx\")\n",
        "\n",
        "    # 3) Define the query\n",
        "    user_query = \"What are the average waiting times in hospitals in Germany by region?\"\n",
        "\n",
        "    # 4) Calculate TF-IDF for that query, saving results with 10 decimal precision\n",
        "    search_engine.calculate_tf_idf_for_query(user_query, page_indexes, \"results.xlsx\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}